# üéØ Gu√≠a para Mejorar la Precisi√≥n del Modelo

## Problemas Identificados

1. **Desbalance de clases**: Algunas clases tienen muy pocos ejemplos (stand_site: 2, walk_away: 6)
2. **Baja precisi√≥n general**: F1 macro ~0.37, Accuracy ~0.57
3. **Falta de datos**: Solo 7 sujetos, algunos con pocas actividades

## Soluciones Implementadas

### 1. Consolidaci√≥n de Clases ‚úÖ

Se agruparon clases similares para reducir el desbalance:

```python
CLASS_MAPPING = {
    "walk_back": "walk",
    "walk_front": "walk",
    "walk_side": "walk",
    "walking_away": "walk",
    "walking_to_camera": "walk",
    "walk_away": "walk",
    "stand_front": "stand",
    "stand_side": "stand",
    "stand_site": "stand",
}
```

**Resultado esperado**: De 11 clases a 4 clases principales, mejorando el balance.

### 2. Feature Engineering Mejorado ‚úÖ

Se a√±adieron caracter√≠sticas adicionales:

- **Ratios corporales**: `hip_height_ratio`, `body_aspect_ratio`
- **Distancias adicionales**: Entre mu√±ecas, tobillos
- **Altura total del cuerpo**: Para normalizaci√≥n mejorada

### 3. Validaci√≥n Leave-One-Subject-Out ‚úÖ

Asegura que el modelo generalice a nuevos sujetos, no solo a nuevos frames.

## Recomendaciones Adicionales

### 1. Recolectar M√°s Datos

**Prioridad: ALTA**

- **M√°s sujetos**: Idealmente 15-20 sujetos diferentes
- **M√°s variaciones**: Diferentes alturas, pesos, edades
- **M√°s perspectivas**: Frontal, lateral, 45 grados
- **M√°s condiciones**: Diferentes iluminaciones, fondos

**C√≥mo hacerlo**:
```bash
# Usar el script para procesar nuevos videos
python process_videos.py --input ruta/nuevos/videos --output Entrega 2/data/poses
```

### 2. Aumento de Datos (Data Augmentation)

**Prioridad: MEDIA**

- **Espejo horizontal**: Duplicar videos reflejados
- **Variaciones de velocidad**: Acelerar/ralentizar videos
- **Ruido en coordenadas**: A√±adir peque√±as variaciones aleatorias a los landmarks
- **Rotaciones menores**: Rotar ligeramente las coordenadas

**Ejemplo de implementaci√≥n**:
```python
# En src/features.py, a√±adir funci√≥n de augmentaci√≥n
def augment_landmarks(df, mirror=True, noise_std=0.01):
    if mirror:
        # Reflejar coordenadas x
        for i in range(33):
            df[f'x_{i}'] = 1.0 - df[f'x_{i}']
    if noise_std > 0:
        # A√±adir ruido gaussiano
        for i in range(33):
            df[f'x_{i}'] += np.random.normal(0, noise_std, len(df))
            df[f'y_{i}'] += np.random.normal(0, noise_std, len(df))
    return df
```

### 3. Ajuste de Hiperpar√°metros M√°s Exhaustivo

**Prioridad: MEDIA**

Ampliar los grids de b√∫squeda:

```python
# En src/models.py
param_grids = {
    "SVM_RBF": {
        "clf__C": [0.1, 1, 3, 10, 30],
        "clf__gamma": ["scale", "auto", 0.001, 0.01, 0.05, 0.1],
    },
    "RandomForest": {
        "n_estimators": [200, 300, 500, 800],
        "max_depth": [None, 10, 15, 20, 25],
        "max_features": ["sqrt", 0.3, 0.5, 0.7],
        "min_samples_leaf": [1, 2, 4],
    },
    "XGBoost": {
        "n_estimators": [100, 200, 300, 500],
        "learning_rate": [0.01, 0.05, 0.1, 0.2],
        "max_depth": [3, 5, 7, 9],
        "subsample": [0.7, 0.8, 0.9, 1.0],
        "colsample_bytree": [0.7, 0.8, 0.9, 1.0],
    },
}
```

### 4. Reducci√≥n de Caracter√≠sticas

**Prioridad: BAJA**

Despu√©s de tener m√°s datos, usar t√©cnicas de selecci√≥n de caracter√≠sticas:

- **Feature importance**: Usar importancia de RandomForest/XGBoost
- **Correlaci√≥n**: Eliminar caracter√≠sticas altamente correlacionadas
- **PCA**: Reducir dimensionalidad manteniendo varianza

### 5. Modelos Secuenciales

**Prioridad: BAJA** (m√°s complejo)

Para capturar dependencias temporales:

- **LSTM**: Para secuencias de frames
- **CNN 1D**: Para patrones temporales en caracter√≠sticas
- **Transformer**: Para atenci√≥n temporal

**Nota**: Requiere m√°s datos y tiempo de entrenamiento.

### 6. Ensambles

**Prioridad: MEDIA**

Combinar predicciones de m√∫ltiples modelos:

```python
from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('svm', svm_model),
        ('rf', rf_model),
        ('xgb', xgb_model)
    ],
    voting='soft'
)
```

### 7. Balanceo de Clases

**Prioridad: MEDIA**

Si despu√©s de consolidar a√∫n hay desbalance:

- **SMOTE**: Generar muestras sint√©ticas de clases minoritarias
- **Undersampling**: Reducir muestras de clases mayoritarias
- **Class weights**: Ajustar pesos en el entrenamiento (ya implementado)

## Plan de Acci√≥n Recomendado

### Fase 1: Datos (1-2 semanas)
1. ‚úÖ Consolidar clases (YA HECHO)
2. Recolectar m√°s videos (5-10 sujetos adicionales)
3. Procesar y a√±adir al dataset

### Fase 2: Mejoras de Modelo (1 semana)
1. Implementar aumentaci√≥n de datos
2. Ampliar grid de hiperpar√°metros
3. Re-entrenar modelos

### Fase 3: Optimizaci√≥n (1 semana)
1. Selecci√≥n de caracter√≠sticas
2. Ensambles
3. Validaci√≥n final

## M√©tricas Objetivo

- **Accuracy**: > 0.75
- **F1 Macro**: > 0.70
- **Balanced Accuracy**: > 0.70
- **Por clase**: Precision y Recall > 0.65 para todas las clases

## Scripts √ötiles

### Ver distribuci√≥n de clases:
```python
import pandas as pd
df = pd.read_csv("Entrega 2/data/poses/features_dataset.csv")
print(df['action'].value_counts())
```

### Verificar calidad de datos:
```python
# Verificar frames v√°lidos por video
summary = pd.read_csv("Entrega 2/data/poses/poses_summary.csv")
print(summary[['video', 'valid_ratio']].sort_values('valid_ratio'))
```

## Notas Finales

- **Paciencia**: Mejorar precisi√≥n requiere tiempo y datos
- **Iteraci√≥n**: Probar una mejora a la vez para entender su impacto
- **Validaci√≥n**: Siempre usar LOSO para evaluar generalizaci√≥n real
- **Documentaci√≥n**: Registrar qu√© cambios mejoran/deterioran resultados

