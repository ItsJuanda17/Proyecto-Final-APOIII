# ğŸ§â€â™€ï¸ Sistema de AnÃ¡lisis de Actividades Humanas

Proyecto Final - Algoritmo y ProgramaciÃ³n III (APO3)  
Universidad ICESI - Facultad de IngenierÃ­a, DiseÃ±o y Ciencias Aplicadas

## ğŸ“‹ DescripciÃ³n

Sistema de software capaz de analizar actividades especÃ­ficas de una persona (caminar hacia la cÃ¡mara, caminar de regreso, sentarse, ponerse de pie) y realizar un seguimiento de movimientos articulares y posturales en tiempo real usando MediaPipe y modelos de Machine Learning.

## ğŸ¯ CaracterÃ­sticas

- **DetecciÃ³n de poses**: ExtracciÃ³n de 33 landmarks corporales usando MediaPipe
- **ClasificaciÃ³n de actividades**: Reconocimiento de actividades usando modelos supervisados (SVM, Random Forest, XGBoost)
- **AnÃ¡lisis postural**: CÃ¡lculo de Ã¡ngulos articulares, inclinaciÃ³n del tronco y velocidades
- **Interfaz en tiempo real**: VisualizaciÃ³n de detecciones usando la cÃ¡mara o videos

## ğŸ—ï¸ Estructura del Proyecto

```
Proyecto-Final-APOIII/
â”‚
â”œâ”€â”€ Entrega 1/                    # Primera entrega
â”‚   â”œâ”€â”€ docs/
â”‚   â””â”€â”€ src/
â”‚
â”œâ”€â”€ Entrega 2/                    # Segunda entrega
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ poses/                # Archivos parquet y CSV
â”‚   â”‚   â””â”€â”€ videos/               # Videos de entrenamiento
â”‚   â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Proyecto.ipynb
â”‚
â””â”€â”€ Entrega 3/                    # Tercera entrega (CÃ³digo reorganizado)
    â”œâ”€â”€ src/                      # CÃ³digo fuente principal
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n global
    â”‚   â”œâ”€â”€ preprocessing.py      # Preprocesamiento de datos
    â”‚   â”œâ”€â”€ features.py           # ExtracciÃ³n de caracterÃ­sticas
    â”‚   â”œâ”€â”€ models.py             # Entrenamiento de modelos
    â”‚   â”œâ”€â”€ inference.py          # Inferencia en tiempo real
    â”‚   â””â”€â”€ app.py                # AplicaciÃ³n principal
    â”‚
    â”œâ”€â”€ models/                   # Modelos entrenados (generado)
    â”‚   â”œâ”€â”€ best_model.pkl
    â”‚   â”œâ”€â”€ confusion_matrix.png
    â”‚   â””â”€â”€ metrics.csv
    â”‚
    â”œâ”€â”€ train.py                  # Script de entrenamiento
    â”œâ”€â”€ process_videos.py         # Script para procesar videos
    â”œâ”€â”€ check_setup.py            # Script de verificaciÃ³n
    â”œâ”€â”€ requirements.txt          # Dependencias
    â”œâ”€â”€ README.md                 # Este archivo
    â”œâ”€â”€ INSTRUCCIONES_USO.md      # GuÃ­a de uso detallada
    â””â”€â”€ MEJORAS_PRECISION.md      # Recomendaciones de mejora
```

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.8 o superior
- pip

### Pasos

1. **Clonar el repositorio** (o descargar el proyecto)

2. **Crear un entorno virtual** (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Uso

### 1. Generar Dataset de CaracterÃ­sticas

**âš ï¸ IMPORTANTE: Ejecuta desde la carpeta `Entrega 3`**

Si ya tienes los archivos `.parquet` con los landmarks extraÃ­dos, puedes generar el dataset de caracterÃ­sticas ejecutando:

```bash
cd "Entrega 3"
python train.py
```

Este script:
- Carga los archivos parquet de `../Entrega 2/data/poses/`
- Preprocesa y extrae caracterÃ­sticas
- Genera `features_dataset.csv` en `../Entrega 2/data/poses/`
- Entrena modelos (SVM, Random Forest, XGBoost)
- EvalÃºa con validaciÃ³n Leave-One-Subject-Out (LOSO)
- Guarda el mejor modelo en `models/best_model.pkl`

### 2. Ejecutar DetecciÃ³n en Tiempo Real

#### Con la cÃ¡mara web:
```bash
cd "Entrega 3"
python -m src.app --camera 0
```

#### Con un video:
```bash
cd "Entrega 3"
python -m src.app --video ruta/al/video.mp4
```

#### Guardar video procesado:
```bash
cd "Entrega 3"
python -m src.app --video entrada.mp4 --output salida.mp4
```

#### Opciones disponibles:
- `--model`: Ruta al modelo (default: `models/best_model.pkl`)
- `--camera`: Ãndice de la cÃ¡mara (default: 0)
- `--video`: Ruta a video para procesar
- `--output`: Ruta para guardar video procesado

#### Controles durante la ejecuciÃ³n:
- `q`: Salir
- `r`: Reiniciar el buffer de frames
- `s`: Guardar screenshot

## ğŸ”§ Mejoras Implementadas

### 1. ReorganizaciÃ³n del CÃ³digo
- âœ… SeparaciÃ³n en mÃ³dulos Python reutilizables
- âœ… ConfiguraciÃ³n centralizada
- âœ… CÃ³digo documentado y mantenible

### 2. Mejora de PrecisiÃ³n
- âœ… **ConsolidaciÃ³n de clases**: Se agruparon clases similares para reducir el desbalance:
  - Variantes de caminar â†’ `walk`
  - Variantes de estar de pie â†’ `stand`
  - Sentarse se mantiene separado por perspectiva (`sit_front`, `sit_side`)
- âœ… **Feature engineering mejorado**:
  - Ãngulos articulares (codos, rodillas, caderas)
  - InclinaciÃ³n del tronco
  - Velocidades de puntos clave
  - Distancias entre articulaciones
  - Ratios corporales (altura/ancho, altura de cadera)
- âœ… **ValidaciÃ³n robusta**: Leave-One-Subject-Out para evitar sobreajuste

### 3. Interfaz en Tiempo Real
- âœ… VisualizaciÃ³n de landmarks en video
- âœ… PredicciÃ³n de actividad en tiempo real
- âœ… Muestra de confianza de la predicciÃ³n
- âœ… Soporte para cÃ¡mara y videos

## ğŸ“ˆ Resultados Esperados

DespuÃ©s de consolidar las clases y mejorar las caracterÃ­sticas, se espera:
- **Mejor precisiÃ³n**: ReducciÃ³n de clases de 11 a 4 principales
- **Mejor balance**: DistribuciÃ³n mÃ¡s equilibrada de muestras por clase
- **Mejor generalizaciÃ³n**: ValidaciÃ³n LOSO asegura que el modelo funciona con nuevos sujetos

## ğŸ› SoluciÃ³n de Problemas

### Error: "Modelo no encontrado"
AsegÃºrate de haber entrenado el modelo primero ejecutando `python train.py` desde la carpeta `Entrega 3`

### Error: "No se pudo abrir la cÃ¡mara"
- Verifica que la cÃ¡mara estÃ© conectada
- Prueba con un Ã­ndice diferente: `--camera 1`
- En Linux, puede requerir permisos: `sudo usermod -a -G video $USER`

### Baja precisiÃ³n en predicciones
- Verifica que haya suficiente iluminaciÃ³n
- AsegÃºrate de que la persona estÃ© completamente visible en el frame
- Considera entrenar con mÃ¡s datos

## ğŸ“ Notas Importantes

1. **Ejecutar desde Entrega 3**: Todos los scripts deben ejecutarse desde la carpeta `Entrega 3` para que las rutas funcionen correctamente.

2. **Datos en Google Drive**: Los datos originales estÃ¡n organizados en Google Drive. AsegÃºrate de tener los archivos `.parquet` en `Entrega 2/data/poses/` antes de entrenar.

2. **ConsolidaciÃ³n de clases**: El sistema consolida automÃ¡ticamente clases similares para mejorar la precisiÃ³n. Ver `src/config.py` para personalizar el mapeo.

3. **Ventana temporal**: El modelo usa ventanas de 32 frames para hacer predicciones. Se necesita al menos 1 segundo de video a 30 FPS.

## ğŸ‘¥ Integrantes

- Juan David Acevedo - A00399081
- Santiago Santacruz - A00378149
- Esteban Cuellar - A00402548

## ğŸ“š Referencias

- [MediaPipe Pose](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=es-419)
- [scikit-learn](https://scikit-learn.org/)
- [XGBoost](https://xgboost.readthedocs.io/)

## ğŸ“„ Licencia

Este proyecto es parte de un trabajo acadÃ©mico de la Universidad ICESI.

---


